---
trigger:
  branches:
    exclude:
      - "*"

pool:
  vmImage: ubuntu-latest

jobs:
  - job: Run_dbt
    displayName: "Build changed dbt models"
    steps:
      
      # Install dbt and python
      - template: template-steps-init-dbt.yml

      # Download previously generated manifest from azure-pipelines-dbt-ci-main.yml
      # Insert the service connection name and storage account details
      # Save the manifest in a temp folder on the build agent
      - task: AzureCLI@2
        displayName: "Download manifest from storage account"
        inputs:
          azureSubscription: "your-azure-service-connection-name"
          scriptType: "bash"
          scriptLocation: "inlineScript"
          inlineScript: |
            # Download the file
            az storage blob download \
              --account-name $STORAGE_ACCOUNT_NAME \
              --container-name $CONTAINER_NAME \
              --file $FILE_PATH \
              --name $BLOB_NAME \
              --auth-mode login \
              --overwrite
        env:
          STORAGE_ACCOUNT_NAME: "your-storage-account-name"
          CONTAINER_NAME: "your-container-name"
          BLOB_NAME: "jaffle-shop/manifest.json"
          FILE_PATH: "$(Agent.TempDirectory)/manifest.json"
      
      # Run dbt build with defer, deferring to the models generated by azure-pipelines-dbt-ci-main.yml
      # Set the connection details environment vars from profiles.yml
      - pwsh: |
          dbt deps
          dbt build --select "state:modified+1" --state "$(Agent.TempDirectory)" --defer
        displayName: "dbt build (defer to ci_main schema)"
        workingDirectory: "$(Build.SourcesDirectory)"
        env:
          DBT_PROFILES_DIR: "$(Build.SourcesDirectory)/.devops-pipelines"
          DBT_DATABRICKS_SCHEMA: "ci"
          DBT_DATABRICKS_HOST: "your-databricks-host"
          DBT_DATABRICKS_HTTP_PATH: "/sql/1.0/warehouses/your-warehouse-id"
          DBT_DATABRICKS_TOKEN: "your-databricks-token"